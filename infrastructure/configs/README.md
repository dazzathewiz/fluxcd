# Infrastructure Configs

## 1Password provider using External Secrets Operator
See: [1Password Connect](../doc-1password)

## Notify and Alerts `notify-slack.yaml`
- A slack URL is configured in 1Password `fluxcd-slackurl`
- The URL secret `fluxcd-slackurl` syncronised from 1Password
- A `slack` provider type is configured
- `on-call-webapp` Alert is configured with default settings

    ### Sources
    [Flux Guide][https://fluxcd.io/flux/guides/notifications/]
    [TechnoTim Guide][https://docs.technotim.live/posts/flux-devops-gitops/]

## Traefik & Cert-Manager

ðŸ“º [Watch the TechnoTim Video](https://www.youtube.com/watch?v=G4CmbYL9UPg)
ðŸ“º [Watch Chistian Lempa's Video](https://www.youtube.com/watch?v=DvXkD0f-lhY)

### [Traefik Dashboard](https://traefik.inf.dazzathewiz.com/)

- A Base64 encoded BasicAuth string is imported via 1Password to `traefik-dashboard-auth`
    Note this is generated by htpasswd `htpasswd -nb <user> <password> | openssl base64`
- `traefik.inf.dazzathewiz.com` is a DNS resolvable name pointing to the trafik Controller `loadBalancerIP`. This can be setup in PiHole via a [custom dnsmasq file](https://brandonrozek.com/blog/wildcarddomainspihole/)
- AWS Route53 AuthID and SecretKey is stored in 1Password as they are both sensitive (including the AuthID). `ClusterIssuers` can be configured with `accessKeyIDSecretRef` and `secretAccessKeySecretRef` respectively.
- Yaml deployment files used in the Traefik and Cert-Manager deployments [traefik cert-manager let's encrypt](https://github.com/techno-tim/launchpad/tree/master/kubernetes/traefik-cert-manager)
- Cert-Manager pods specifically reach out to external DNS using `values:` to solve problems when internal split-dns is available w/ different IP resolution to the internet DNS:
    ```
    extraArgs:
    - --dns01-recursive-nameservers=1.1.1.1:53,9.9.9.9:53
    - --dns01-recursive-nameservers-only
    podDnsPolicy: None
    podDnsConfig:
    nameservers:
        - 1.1.1.1
        - 9.9.9.9
    ```
- Useful commands to inspect certificates:
    - `kubectl get certificate --all-namespaces`
    - `kubectl describe certificate --all-namespaces`
    - `kubectl get challenges --all-namespaces ` -> will only show challenges currently in progress of being validated.

    ### Sources
    [FluxCD example of cert-manager helm release][https://geek-cookbook.funkypenguin.co.nz/kubernetes/ssl-certificates/cert-manager/]
    [Cert-Manager ClusterIssuer manifests][https://docs.gitops.weave.works/docs/guides/cert-manager/]
    [Certificate Generation](https://geek-cookbook.funkypenguin.co.nz/kubernetes/ssl-certificates/wildcard-certificate/)
    [Traefik Cookbook](https://geek-cookbook.funkypenguin.co.nz/kubernetes/ingress/traefik/)

## Rancher

ðŸ“º [Watch the TechnoTim Video](https://www.youtube.com/watch?v=APsZJbnluXg)

### [Rancher Dashboard](https://rancher.inf.dazzathewiz.com/)

### Requirements
- Have emberstack/reflector installed to sync `cert-manager` [secrets across namespaces](https://cert-manager.io/docs/tutorials/syncing-secrets-across-namespaces/)
- Configure a `Certificate` secret and ensure it's synced to the `cattle-system` namespace
- Include the `hostname:` as a subdomain of the traefik Controller loadBalancerIP

## Longhorn

ðŸ“º [Watch the TechnoTim Video](https://www.youtube.com/watch?v=eKBBHc0t7bc)

### [Longhorn Dashboard](https://longhorn.inf.dazzathewiz.com/)

[Helm Chart values.yaml](https://github.com/longhorn/longhorn/blob/master/chart/values.yaml)
```
  values:
    defaultSettings:
      backupTarget: s3://k3s-backup@ap-south-1/                         # See: [Backup to Linode](#backup-to-linode)
      backupTargetCredentialSecret: linode-k3s-backup-bucket-secret     # See: [Backup to Linode](#backup-to-linode)
      createDefaultDiskLabeledNodes: true                               # See: [Storage Node Selection](#storage-node-selection)
      defaultDataPath: /mnt/nvme0/                                      # See: [Storage Node Selection](#storage-node-selection)
```

### Post Install (Manual Configuration)
After the installation of Longhorn, I've needed to:
1. Change the default StorageClass within the cluster to be `longhorn-diskencrypt-global` instead of built-in `local-path` 
    `kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'`
    `kubectl patch storageclass longhorn -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'`
2. Update the `Reserved` storage on each node; Node -> (scroll to right) Node Operation -> Edit nodes and disks -> Change: `Storage Reserved`

### Backup to Linode
See: [Creating a Backup Target](https://fredrickb.com/2022/07/24/introducing-longhorn-to-the-homelab/#creating-backup-target)
1. Configure a Kubernetes Secret (Note this repo will use ExternalSecrets)
    ```
    apiVersion: v1
    kind: Secret
    metadata:
    name: longhorn-linode-s3-backup-credentials
    namespace: longhorn-system
    type: Opaque
    data:
    AWS_ENDPOINTS: "<s3-endpoint-excluding-bucket-name>" # https://<linode-region>.linodeobjects.com
    AWS_ACCESS_KEY_ID: "<access-key>" # Access Key connected to given bucket in Linode
    AWS_SECRET_ACCESS_KEY: "<access-secret>" # Access Secret connected to given bucket in Linode
    ```
    - The secret name becomes the value of `backupTargetCredentialSecret:` in Longhorn Helm deployment
2. The S3 formatted URL (`s3://<bucket-name>@<placeholder-region>/`)
    - This URL becomes the value of `backupTarget:` in Longhorn Helm deployment
    - URL format can be tricky to get right when using S3 compatible services outside of AWS. 
        - [Setting a backup target](https://longhorn.io/docs/1.4.0/snapshots-and-backups/backup-and-restore/set-backup-target/#enable-virtual-hosted-style-access-for-s3-compatible-backupstore)
        - [Git BUG: support non aws s3 buckets](https://github.com/longhorn/longhorn/issues/4205)

### Backup/snapshot scheduling
[Recurring Snapshots and Backups](https://longhorn.io/docs/1.4.1/snapshots-and-backups/scheduling-backups-and-snapshots/)
- `snapshot` takes a snapshot within Longhorn storage
- `backup` sends a backup to the condigured [backup store](#backup-to-linode)

    1. Take a `snapshot` locally every 12 hours and keep 4 versions (IE: 2 days)
    ```
    ---
    apiVersion: longhorn.io/v1beta1
    kind: RecurringJob
    metadata:
    name: 12-hour-snapshots
    namespace: longhorn-system
    spec:
    cron: "0 */12 * * *"
    task: "snapshot"
    groups:
    - default
    retain: 4
    concurrency: 2
    labels:
        job: 12-hour-snapshots
    ```
    2. Take a daily `backup` to remove backup store at 11am and keep 3 versions (IE: 3 days)
    ```
    ---
    apiVersion: longhorn.io/v1beta1
    kind: RecurringJob
    metadata:
    name: daily-backups
    namespace: longhorn-system
    spec:
    cron: "0 11 * * *"
    task: "backup"
    groups:
    - default
    retain: 3
    concurrency: 2
    labels:
        job: daily-backups
    ```

### Storage Node Selection 

The Longhorn setting `Create Default Disk on Labeled Nodes` is enabled so k3s selects only labeled nodes for scheduling storage. 
This is enabled at install of Longhorn Helm Chart with option `createDefaultDiskLabeledNodes: true`. See: 
[Customizing Default Disks for New Nodes](https://longhorn.io/docs/1.4.1/advanced-resources/default-disk-and-node-config/#customizing-default-disks-for-new-nodes)

1. Apply a label to storage nodes: `node.longhorn.io/create-default-disk=true`
2. Change the `settings.default-data-path` in Helm config `defaultDataPath: /mnt/nvme0/`

Note the node label is done automatically, and path `/mnt/nvme0/` automatically mounted when deploying k3s with 
[dazzathewiz/k3s-ansible](https://github.com/dazzathewiz/k3s-ansible)

### Volume Encryption

[Volume Encryption](https://longhorn.io/docs/1.4.1/advanced-resources/security/volume-encryption/) is configured
by creating the `longhorn-diskencrypt-global` StorageClass. PVC's should reference this class to create encrypted volumes.
Note that [Automated Backups](#backupsnapshot-scheduling) are scheduled to external providers, so it's important to encrypt
volumes containing anything sensitive as per the [Longhorn documentation](https://longhorn.io/blog/longhorn-v1.2/#encryption-volume-and-backup).

### Sources
- [Great example of configuration in homelab](https://fredrickb.com/2022/07/24/introducing-longhorn-to-the-homelab/)
- [Longhorn UI with Traefik](https://tansanrao.com/guide-storage-ingress-webui-k8s/)
- [Longhorn defaul disk labelling](https://longhorn.io/kb/tip-only-use-storage-on-a-set-of-nodes/#tell-longhorn-to-create-a-default-disk-on-a-specific-set-of-nodes)
- [Configuring Defaults for Nodes and Disks](https://longhorn.io/docs/1.4.1/advanced-resources/default-disk-and-node-config/)

## Intel GPU resource and node labelling

[Plex on Kubernetes with intel iGPU passthrough](https://www.reddit.com/r/selfhosted/comments/121vb07/plex_on_kubernetes_with_intel_igpu_passthrough/)

### node-feature-discovery
Use Node Featrure Discovery (NFD) to detect hardware features - including the Intel i915 iGPU
- [NFD Helm Deployment](https://kubernetes-sigs.github.io/node-feature-discovery/master/deployment/helm.html)
- Assists with GPU PCI Device labelling in cluster EG: Label: `feature.node.kubernetes.io/pci-0300_8086_3e92.present=true`

    #### PCI Feature Labels
    The defafult [PCI Feature Labels](https://kubernetes-sigs.github.io/node-feature-discovery/v0.12/reference/worker-configuration-reference.html#sourespcidevicelabelfields)
    will only include the `<class>_<vendor>` label. If we see the example below, this doesn't allow for differentiation of different devices available in the cluster:
    - Node 1 `00:02.0 VGA compatible controller [0300]: Intel Corporation RocketLake-S GT1 [UHD Graphics 750] [8086:4c8a] (rev 04)`
    - Node 2 `00:11.0 VGA compatible controller [0300]: Intel Corporation UHD Graphics 630 (Desktop) [8086:3e92]`
    - Node 3 `00:11.0 VGA compatible controller [0300]: Intel Corporation HD Graphics 530 [8086:1912] (rev 06)`
    To work around this problem, we add NFD-worker configuration for [sources.pci](https://kubernetes-sigs.github.io/node-feature-discovery/v0.12/reference/worker-configuration-reference.html#sourespci)
    to be:
    ```
    sources:
      pci:
        deviceLabelFields: [class, vendor, device]
    ```

    See how to define this in `values:` of the [Helm Release](https://github.com/kubernetes-sigs/node-feature-discovery/blob/master/deployment/helm/node-feature-discovery/values.yaml)

    Review a list of [PCI IDs](https://pci-ids.ucw.cz/v2.2/pci.ids) values

### Intel Device Features
The Intel plugins are available as [Helm Charts](https://github.com/intel/helm-charts):
- [Intel GPU Device Plugin Helm Chart](https://github.com/intel/helm-charts/tree/main/charts/gpu-device-plugin)
- [Intel Device Plugins Operator Helm Chart](https://github.com/intel/helm-charts/tree/main/charts/device-plugin-operator)

    #### NodeFeatureRules
    The `NodeFeatureRule` spec will apply a label `"intel.feature.node.kubernetes.io/gpu": "true"` to nodes containing an Intel GPU. 
    This is required for the scheduling of intel-gpu-plugin pods which allow the intel gpu resource to be consumed by pod deployments.

    Rather than manually applying node-feature-rules from the Intel repo, instead specify in the `HelmRelease` of `intel-device-plugins-gpu`
    a custom value: `nodeFeatureRule: true`:
    ```
    apiVersion: helm.toolkit.fluxcd.io/v2beta1
    kind: HelmRelease
    metadata:
    name: intel-device-plugins-gpu
    namespace: flux-system
    spec:
    interval: 5m
    releaseName: intel-device-plugins-gpu
    targetNamespace: intel-device
    install:
        createNamespace: true
    chart:
        spec:
        chart: intel-device-plugins-gpu
        sourceRef:
            kind: HelmRepository
            name: intel
            namespace: flux-system
        interval: 1m
    timeout: 5m
    values:
        nodeFeatureRule: true
    ```

    #### Scheduling pods with gpu resource
    Use the following pod specification:
    ```
    resources: 
        requests: 
            gpu.intel.com/i915: "1" 
        limits: 
            gpu.intel.com/i915: "1" 
    ```

## Rook Ceph External Cluster
Refer: [rook-ceph-external](./rook-ceph-external)

## Sources
- [intel-device-plugins-for-kubernetes](https://github.com/intel/intel-device-plugins-for-kubernetes)
- [node-feature-discovery](https://github.com/kubernetes-sigs/node-feature-discovery)
